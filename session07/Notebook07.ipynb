{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b6d3e8a",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sympy\n",
    "from sympy import Matrix, Rational, sqrt, symbols, zeros\n",
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d690f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mathematics for Machine Learning\n",
    "\n",
    "## Session 07: Orthogonal projection; the determinant\n",
    "\n",
    "## Gerhard JÃ¤ger\n",
    "\n",
    "### November 12, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa45792e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Orthogonality\n",
    "\n",
    "Recall: vectors $\\mathbf v$ and $\\mathbf w$ are **orthogonal** if and only if\n",
    "\n",
    "$$\n",
    "\\mathbf v^T \\mathbf w = \\mathbf 0\n",
    "$$\n",
    "\n",
    "#### Examples\n",
    "\n",
    "- $\\begin{bmatrix}1\\\\1 \\\\0\\end{bmatrix}$, $\\begin{bmatrix}0\\\\0 \\\\1\\end{bmatrix}$\n",
    "\n",
    "\n",
    "- $\\begin{bmatrix}1\\\\1 \\end{bmatrix}$, $\\begin{bmatrix}2\\\\-2\\end{bmatrix}$\n",
    "\n",
    "\n",
    "- $\\begin{bmatrix}1\\\\1 \\\\ 2\\end{bmatrix}$, $\\begin{bmatrix}-2\\\\-2\\\\2\\end{bmatrix}$\n",
    "\n",
    "- .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c81157a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### orthogonal spaces\n",
    "\n",
    "Two vector spaces $\\mathbf V$ and $\\mathbf W$ are orthogonal if and only if\n",
    "\n",
    "$$\n",
    "\\forall \\mathbf v\\in \\mathbf V, \\mathbf w \\in \\mathbf W. \\mathbf v^T\\mathbf w = \\mathbf 0\n",
    "$$\n",
    "\n",
    "**Examples**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf V &= \\{\\begin{bmatrix}x\\\\0\\end{bmatrix}: x \\in \\mathbb R\\}\\\\[1em]\n",
    "\\mathbf W &= \\{\\begin{bmatrix}0\\\\y\\end{bmatrix}: y \\in \\mathbb R\\}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "These are of course the $x$-axis and $y$-axis of a 2d-space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03b3386",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf V &= \\{\\begin{bmatrix}x\\\\y\\\\0\\end{bmatrix}: x,y \\in \\mathbb R\\}\\\\[1em]\n",
    "\\mathbf W &= \\{\\begin{bmatrix}0\\\\0\\\\z\\end{bmatrix}: z \\in \\mathbb R\\}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "These are the $x$-$y$ plane and the $z$-axis of a 3d-space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba2fcd4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf V &= \\mathrm{span}(\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "-1\\\\\n",
    "0\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "1\\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    ")\\\\[1em]\n",
    "\\mathbf W &= \\mathrm{span}(\\begin{bmatrix}-1\\\\-1\\\\2\\end{bmatrix})\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "How do we know whether $\\mathbf V$ and $\\mathbf W$ are orthogonal?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bfbf31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Observation** Let $V$ and $W$ be two sets of vectors $\\subseteq \\mathbb R^n$. \n",
    "\n",
    "$\\mathrm{span}(V)$ is orthogonal to $\\mathrm{span}(W)$ if and only if for all $\\mathbf v\\in V, \\mathbf w \\in W$: $\\mathbf v$ and $\\mathbf w$ are orthogonal.\n",
    "\n",
    "*Proof*\n",
    "\n",
    "Suppose $\\mathrm{span}(V)$ is orthogonal to $\\mathrm{span}(W)$. If $\\mathbf v\\in V$, then $\\mathbf v\\in\\mathrm{span}(V)$, and likewise for $\\mathbf w$. Hence $\\mathbf v$ and $\\mathbf w$ are orthogonal.\n",
    "\n",
    "Now suppose for all $\\mathbf v\\in V, \\mathbf w \\in W$: $\\mathbf v$ and $\\mathbf w$ are orthogonal. Let $\\mathbf x\\in\\mathrm{span}(V)$ and $\\mathbf y\\in\\mathrm{span}(W)$.\n",
    "\n",
    "If $\\mathbf x\\in\\mathrm{span}(V)$ and $\\mathbf y\\in\\mathrm{span}(W)$, $\\mathbf x = \\sum_i r_i\\mathbf v_i$, $\\mathbf y = \\sum_j s_j\\mathbf w_j$ for $r_1,\\ldots,r_{|V|}, s_1,\\ldots,s_{|W|}\\in \\mathbb R$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf x^T\\mathbf y &= (\\sum_i r_i\\mathbf v_i)^T(\\sum_j s_i\\mathbf w_j)\\\\\n",
    "        &= \\sum_i (r_i\\mathbf v_i)^T(\\sum_j s_j\\mathbf w_j)\\\\\n",
    "        &= \\sum_i \\sum_j(r_i\\mathbf v_i)^T( s_j\\mathbf w_j)\\\\\n",
    "        &= \\sum_i \\sum_jr_i\\mathbf v_i^T( s_j\\mathbf w_j)\\\\\n",
    "        &= \\sum_i \\sum_jr_is_j\\mathbf v_i^T\\mathbf w_j\\\\\n",
    "        &= \\sum_i \\sum_jr_is_j\\mathbf 0\\\\\n",
    "        &= \\mathbf 0\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\\dashv$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc598cd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Observation**\n",
    "\n",
    "Let $A$ be an $m\\times n$ matrix. Then\n",
    "\n",
    "- the column space $C(A)$ is orthogonal to the left null space $C(A^T)$, and\n",
    "- the row space $C(A^T)$ is orthogonal to the null space $N(A)$.\n",
    "\n",
    "*Proof*\n",
    "\n",
    "The column space of $A$ is $\\mathrm{span}(\\{\\mathbf a_i|1\\leq i \\leq n\\})$. If $\\mathbf x$ is in the left null space of $A$, this means that\n",
    "\n",
    "$$\n",
    "A^T \\mathbf x = \\mathbf 0\n",
    "$$\n",
    "\n",
    "It follows that \n",
    "\n",
    "$$\n",
    "\\forall i:\\mathbf a_i^T\\mathbf x = \\mathbf 0\n",
    "$$\n",
    "\n",
    "Due to the previous observation, it follows that $C(A)$ is orthogonal to $N(A^T)$. \n",
    "\n",
    "The proof of the second statement is analogous.\n",
    "\n",
    "$\\dashv$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0c7df2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Orthogonal projections\n",
    "\n",
    "Suppose we have two vectors $\\mathbf a$ and $\\mathbf b$. We want to find the *orthogonal projection from $\\mathbf a$ onto the line through $\\mathbf b$*. This is a vector $\\mathbf p$ with the properties:\n",
    "\n",
    "- $\\mathbf p = x\\mathbf b$ ($\\mathbf p$ lies on the line defined by $\\mathbf a$)\n",
    "- $\\mathbf a - \\mathbf p$ is orthogonal to $\\mathbf b$\n",
    "\n",
    "Here is how we find $\\mathbf p$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(\\mathbf a - x\\mathbf b)^T\\mathbf b &= 0\\\\\n",
    "(\\mathbf a^T - x\\mathbf b^T)\\mathbf b &= 0\\\\\n",
    "\\mathbf a^T\\mathbf b - x\\mathbf b^T\\mathbf b &= 0\\\\\n",
    "\\mathbf a^T\\mathbf b &= x\\mathbf b^T\\mathbf b\\\\\n",
    "x &= \\frac{\\mathbf a^T\\mathbf b}{\\mathbf b^T\\mathbf b}\\\\\n",
    "\\mathbf p &= \\frac{\\mathbf a^T\\mathbf b}{\\mathbf b^T\\mathbf b}\\mathbf b\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $\\mathbf p$ is called the *projection of $\\mathbf a$ onto the line throuhg $\\mathbf b$*.\n",
    "- $\\mathbf e = \\mathbf a - \\mathbf p$ is called the *error*.\n",
    "- $\\mathbf p$ is the point on the line through $\\mathbf b$ which is closest to $\\mathbf a$, i.e., the point which minimizes the error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23861d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Orthogonal projections\n",
    "\n",
    "Now suppose we have a matrix $A$ and a vector $\\mathbf b$, and we want to find the *orthogonal projection of  $\\mathbf b$ onto the* ***column space*** of $A$.\n",
    "\n",
    "In other words, we want to find the point $\\mathbf p$ which\n",
    "\n",
    "- is in the column space of $A$, and\n",
    "- minimizes the error $\\mathbf b-\\mathbf p$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0186408",
   "metadata": {},
   "source": [
    "<img src=\"_img/projection.svg\"  width=\"1000\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "\n",
    "(image from https://medium.com/linear-algebra/part-17-projections-122aac21b07c)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a8467e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- assumptions:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A\\mathbf x &= \\mathbf p\\\\\n",
    "\\mathbf p + \\mathbf e &= \\mathbf b\\\\\n",
    "A^T\\mathbf e &= \\mathbf 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- finding the solution\n",
    "\n",
    "Let us assume that the columns of $A$ are independent. (If this is not the case, we can replace $A$ by some basis of $C(A)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c9de6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Observation** $(A^TA)$ is invertible if and only if the columns of $A$ are independent.\n",
    "\n",
    "*Proof*\n",
    "\n",
    "\n",
    "Suppose $(A^TA)$ is invertible, and let $A\\mathbf x = \\mathbf 0$. Then it follows\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A^TA\\mathbf x &= A^T\\mathbf 0\\\\\n",
    "A^TA\\mathbf x &= \\mathbf 0\\\\\n",
    "\\mathbf x &= (A^TA)^{-1}\\mathbf 0\\\\\n",
    "&= \\mathbf 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "This entails that the columns of $A$ are independent.\n",
    "\n",
    "Now suppose the columns of $A$ are independent. The Gauss-Jordan elimination factorizes\n",
    "\n",
    "$$\n",
    "A^T = E R,\n",
    "$$\n",
    "where $E$ is the combined elimination matrix and $R$ is the reduced row echelon form of $A^T$.\n",
    "\n",
    "As shown earlier, $E$ is invertible.\n",
    "\n",
    "If the columns of $A$ are independent, $R$ contains $n$ pivot columns, and no free column. It follows that\n",
    "\n",
    "$$\n",
    "R^T R = \\mathbf I,\n",
    "$$\n",
    "\n",
    "since the dot product of a pivot column with itself must be $1$, and the dot product of two different pivot columns must be $0$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f3273f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then we have:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A^TA &= ERR^T E^T\\\\\n",
    "&= E~\\mathbf I~ E^T\\\\\n",
    "&= E E^T\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "By construction, $E$ is invertible. Therefore\n",
    "\n",
    "$$\n",
    "(A^TA)^{-1} = (E^{-1})^T E^{-1}\n",
    "$$\n",
    "\n",
    "$\\dashv$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b67139",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- deriving the solution:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A\\mathbf x &= \\mathbf p\\\\\n",
    "\\mathbf p + \\mathbf e &= \\mathbf b\\\\\n",
    "A^T\\mathbf e &= \\mathbf 0\\\\\n",
    "A^T\\mathbf b &= A^T\\mathbf p + A^T\\mathbf e\\\\\n",
    "A^T\\mathbf b &= A^T\\mathbf p\\\\\n",
    "&= A^T A\\mathbf x\\\\\n",
    "\\mathbf x &= (A^T A)^{-1}A^T\\mathbf b\\\\\n",
    "\\mathbf p &= A(A^T A)^{-1}A^T\\mathbf b\\\\\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febb16e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Projection matrix\n",
    "\n",
    "The matrix\n",
    "\n",
    "$$\n",
    "P = A(A^TA)^{-1}A^T\n",
    "$$\n",
    "\n",
    "is the **projection matrix** that maps each vector to its projection onto the column space of $A$.\n",
    "\n",
    "Each projection matrix $P$ has the property that $PP=P$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P &= A(A^TA)^{-1}A^T\\\\\n",
    "PP &= A(A^TA)^{-1}A^TA(A^TA)^{-1}A^T\\\\\n",
    "&= A(A^TA)^{-1}(A^TA)(A^TA)^{-1}A^T\\\\\n",
    "&= A(A^TA)^{-1}A^T\\\\\n",
    "&= P\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c215042a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Statistics interlude: Linear regression\n",
    "\n",
    "**linear regression**\n",
    "\n",
    "- independent variables: $m\\times n$ matrix $X$\n",
    "    - $n$: number of observations\n",
    "    - $m$: number of independent variables\n",
    "- dependent variable: length-$n$ vector $\\mathbf y$\n",
    "- goal: find parameter vector $\\beta$ (length $m+1$) such that the *total squared error* is minimized\n",
    "\n",
    "$$\n",
    "\\hat\\beta = \\arg_\\beta\\min \\sum_i (\\beta_1 + \\sum_{j=1}^m \\beta_{j+1}x_{i,j} - y_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39345f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's rephrase this with linear algebra\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X_1 &= [\\mathbf 1 X]\\\\\n",
    "\\hat {\\mathbf y} &= X_1\\beta\\\\\n",
    "\\epsilon &= ||\\mathbf y - \\hat {\\mathbf y}||^2\\\\\n",
    "\\hat\\beta &= \\arg_\\beta\\min \\epsilon\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "From the second equation we see that $\\hat {\\mathbf y}$ is in the column space of $X_1$. \n",
    "\n",
    "The goal is to find the point $\\hat {\\mathbf y}$ in the column space of $X_1$ that minimizes the squared distance to $\\mathbf y$.\n",
    "\n",
    "This is also the point that minimizes the absolute distance between $\\hat {\\mathbf y}$ and $\\mathbf y$. In other words, $\\hat {\\mathbf y}$ is the projection of $\\mathbf y$ onto the column space of $X_1$:\n",
    "\n",
    "$$\n",
    "\\hat\\beta = (X_1^TX_1)^{-1}X_1^T\\mathbf y\n",
    "$$\n",
    "\n",
    "If the columns of $X_1$ are not independent, $\\hat\\beta$ is not well-defined (and your statistics software will complain).\n",
    "\n",
    "https://book.stat420.org/collinearity.html"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
